{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating Classification Models of Personalized Persuasive Dialogue Systems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Goal:\n",
    "## 1 - Generalized combination of strategies that leads to successful persuasion / dialogue (Measured by donation in dataset)\n",
    "\n",
    "## 2 - Importance of pysch profiles in determining approach to persuasion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Work Log:\n",
    "## July 16, 2021:\n",
    "* Initialized Pandas data frames one containing persuader strategies used, and the other whether or not the dialogue ended in a donation (i.e. Successful dialogue or not). \n",
    "  * Both of these data frames are linked by specific dialogue identification numbers. \n",
    "* Successfully implemented an elementary 'features' method.\n",
    "* Successfully implemented a simple Naive Bayes' Classifier that uses the 'features' method.\n",
    "\n",
    "## July 17, 2021:\n",
    "* Experimented with features\n",
    "  * Concluded that setting features manually had next to no effect in classification model\n",
    "  * Without any features the model had avg 60% of predicting correct\n",
    "* Decided a BLSTM model would be the way to go\n",
    "\n",
    "## July 18, 2021:\n",
    "* Re-Worked data initialization feature to better suit PyTorch\n",
    "  * Created a method for generating training and testing data with crucial features (i.e. Dialogue ID, Strategies Used, Score)\n",
    "* Began work on BRNN implementation\n",
    "  * Impelemented first version of BRNN class\n",
    "\n",
    "## July 20, 2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "''' Imports '''\n",
    "\n",
    "# General Imports\n",
    "from os import path\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk\n",
    "from nltk.classify import apply_features\n",
    "\n",
    "# NumPy Imports\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "from torchtext.legacy import data\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "''' Generating Data '''\n",
    "\n",
    "\n",
    "def generate_json_data():\n",
    "    '''\n",
    "    Method Info - generate_json_data() :\n",
    "    check if \"persuasion_data.json\" file already exists\n",
    "    import data from .xlsx files\n",
    "    create two Pandas dataframes\n",
    "    create a list of tuples containing dialogue ID and score from one of the dataframes\n",
    "    create a dictionary using dialogue IDs as keys and list of strategies used in the dialogue as values\n",
    "    generate a .json file to hold data for torchtext processing in format {\"Dialogue ID\": ID str, \"Strategies\": Strategies list, \"Score\": score int}\n",
    "    '''\n",
    "\n",
    "    if path.exists(\"test_data.json\") and path.exists(\"train_data.json\"):\n",
    "        print(\"Files already exists\")\n",
    "    else:\n",
    "\n",
    "        ''' Initializing ID and Score data from 'info.xlsx' '''\n",
    "\n",
    "        info = pd.read_excel(r'info.xlsx')\n",
    "        info = info[info.B4 != 1]\n",
    "        info = info.reset_index()\n",
    "\n",
    "        # checking if donation was made to create binary score scheme\n",
    "        score = []\n",
    "        for row in info.itertuples():\n",
    "            if row[6] > 0:\n",
    "                score.append(1)\n",
    "            else:\n",
    "                score.append(0)\n",
    "        info['Score'] = score\n",
    "\n",
    "        # dropping unnecessary columns from dataframe\n",
    "        info.drop(['index', 'B3', 'B4', 'B5', 'B6', 'B7'],\n",
    "                  inplace=True, axis=1)\n",
    "\n",
    "        # creating finished id_and_score list of tuples using generator\n",
    "        id_and_score = sorted([(row[1], row[2]) for row in info.itertuples()])\n",
    "\n",
    "        ''' Initializing ID and Strategy data from 'xlsx' file in the form of Pandas Dataframe '''\n",
    "\n",
    "        main = pd.read_excel(r'dialog.xlsx')\n",
    "        main = main[main.B4 != 1]\n",
    "        main = main.reset_index()\n",
    "        main.drop(['Unnamed: 0', 'index', 'B4', 'Turn', 'Unit', 'ee_label_1',\n",
    "                   'ee_label_2', 'er_label_2', 'neg', 'neu', 'pos'], inplace=True, axis=1)\n",
    "\n",
    "        ''' Transforming Data from Pandas Dataframe to Python Dictionary '''\n",
    "\n",
    "        dialogue_IDs = []           # list of all dialogue IDs\n",
    "        strats = []                 # temp list used for strats\n",
    "        id_and_strat = {}           # dict for id and strat\n",
    "\n",
    "        # iterating through main dataframe\n",
    "        for pos, row in enumerate(main.itertuples()):\n",
    "\n",
    "            if row[1] not in dialogue_IDs:\n",
    "                # appending IDs to dialogue IDs list\n",
    "                dialogue_IDs.append(row[1])\n",
    "\n",
    "                # ID access mechanism\n",
    "                current_ID = dialogue_IDs[-1]\n",
    "                if len(dialogue_IDs) > 1:\n",
    "                    prev_ID = dialogue_IDs[-2]\n",
    "                else:\n",
    "                    prev_ID = dialogue_IDs[-1]\n",
    "\n",
    "                # appending strategy list to dictionary\n",
    "                strats.insert(0, '<START>')\n",
    "                strats.append('<END>')\n",
    "                s = strats\n",
    "                strats = []\n",
    "                id_and_strat[prev_ID] = s\n",
    "\n",
    "            # mechanism for last dialogue\n",
    "            elif pos == (len(main)-1):\n",
    "\n",
    "                strats.insert(0, '<START>')\n",
    "                strats.append('<END>')\n",
    "                s = strats\n",
    "                strats = []\n",
    "                id_and_strat[current_ID] = s\n",
    "\n",
    "            # appending persuasion strategies to strategy list\n",
    "            if current_ID == row[1]:\n",
    "                strats.append(row[2])\n",
    "\n",
    "        ''' Formatting and Outputing it to a .json file '''\n",
    "\n",
    "        jsonFile1 = open(\"test_data.json\", \"w\")\n",
    "        jsonFile2 = open(\"train_data.json\", \"w\")\n",
    "        for i, (n, s) in enumerate(id_and_score):\n",
    "            aDict = {\"Identification\": n,\n",
    "                     \"Content\": id_and_strat[n], \"Score\": s}\n",
    "            jsonString = json.dumps(aDict)\n",
    "            # if i == 0:\n",
    "            #     jsonFile1.write('{\\n')\n",
    "            #     jsonFile2.write('{\\n')\n",
    "\n",
    "            if i < (len(id_and_score)/2):\n",
    "                jsonFile1.write(jsonString + \"\\n\")\n",
    "            # elif i == (len(id_and_score)-1):\n",
    "            #     jsonFile1.write(jsonString + '\\n}')\n",
    "            #     jsonFile2.write(jsonString + '\\n}')\n",
    "            elif i >= (len(id_and_score)/2):\n",
    "                jsonFile2.write(jsonString + \"\\n\")\n",
    "\n",
    "        jsonFile1.close()\n",
    "        jsonFile2.close()\n",
    "\n",
    "        ''' Output Message '''\n",
    "        print(\"Files now exist\")\n",
    "\n",
    "\n",
    "# calling function\n",
    "generate_json_data()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "''' Settings '''\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 30\n",
    "sequence_length = 1\n",
    "num_layers = 2\n",
    "hidden_size = 15  # expected input features\n",
    "num_classes = 31\n",
    "learning_rate = 0.00005\n",
    "batch_size = 1\n",
    "num_epochs = 11\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "''' Loading Data '''\n",
    "\n",
    "# setting fields\n",
    "# ide = data.Field(use_vocab=True)\n",
    "content = data.Field(sequential=True, use_vocab=True)\n",
    "score = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "fields = {\"Content\": ('c', content), \"Score\": ('s', score)}\n",
    "# \"Identification\": ('i', ide),\n",
    "# importing data from json files\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path='',\n",
    "    train=\"train_data.json\",\n",
    "    test=\"test_data.json\",\n",
    "    format=\"json\",\n",
    "    fields=fields\n",
    ")\n",
    "\n",
    "\n",
    "# building vocabulary\n",
    "content.build_vocab(train_data,\n",
    "                    max_size=1000,\n",
    "                    min_freq=1)\n",
    "\n",
    "print(len(content.vocab))\n",
    "\n",
    "score.build_vocab(train_data,\n",
    "                  max_size=2,\n",
    "                  min_freq=1)\n",
    "# setting iterators\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    device=device)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "''' Creating a bidirectional LSTM '''\n",
    "\n",
    "\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear((hidden_size*2), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Defining Checkpoint Methods\n",
    "\n",
    "def save_checkpoint(state, filename='my_checkpoint.pth.tar'):\n",
    "    print('=> Saving checkpoint')\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Initilaizing network\n",
    "\n",
    "model = BRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "''' Checking Accuracy '''\n",
    "\n",
    "\n",
    "def check_accuracy(iterator, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            x = batch.c.to(device).squeeze(1)\n",
    "            x = nn.functional.pad(x, (0, input_size-len(x)), 'constant', 99)\n",
    "            x = data.reshape(batch_size, sequence_length,\n",
    "                             input_size).to(device)\n",
    "\n",
    "            y = batch.s.to(device)\n",
    "\n",
    "            scores = model(torch.tensor(x, dtype=torch.float))\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    # Toggle model back to train\n",
    "    model.train()\n",
    "    return num_correct / num_samples\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "''' Training the Network '''\n",
    "\n",
    "total_step = len(train_iterator)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(tqdm(train_iterator)):\n",
    "\n",
    "        # Data Shape = [batch_size, seq_len, input_size]\n",
    "        # num_sample\n",
    "        # Get data to cuda if possible\n",
    "        data = batch.c.to(device).squeeze(1)\n",
    "        data = nn.functional.pad(\n",
    "            data, (0, input_size-len(data)), 'constant', 99)\n",
    "        data = data.reshape(batch_size, sequence_length, input_size).to(device)\n",
    "        targets = batch.s.to(device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(torch.tensor(data, dtype=torch.float))\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent update step/adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "print(f\"Learning Rate: {learning_rate} Accuracy on training set: {check_accuracy(train_iterator, model)*100:2f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/var/folders/ml/658yr2r15kj4h626v3ttkx7w0000gn/T/ipykernel_80669/637216790.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores = model(torch.tensor(data, dtype=torch.float))\n",
      "100%|██████████| 150/150 [00:00<00:00, 264.24it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 502.06it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 466.12it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 419.59it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 500.86it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 494.10it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 489.27it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 435.92it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 378.33it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 414.24it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 448.70it/s]\n",
      "/var/folders/ml/658yr2r15kj4h626v3ttkx7w0000gn/T/ipykernel_80669/1840206653.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores = model(torch.tensor(x, dtype=torch.float))\n",
      " 25%|██▌       | 38/150 [00:00<00:00, 378.53it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 1e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 427.87it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 482.04it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 486.53it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 460.11it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 366.13it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 487.87it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 464.86it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 504.68it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 510.63it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 479.51it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 421.43it/s]\n",
      " 31%|███       | 46/150 [00:00<00:00, 452.73it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 2e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 478.53it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 479.70it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 499.24it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 503.21it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 449.45it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 476.84it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 469.65it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 469.42it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 476.18it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 502.33it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 503.85it/s]\n",
      " 69%|██████▉   | 104/150 [00:00<00:00, 519.21it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 3.0000000000000004e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 505.97it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 516.66it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 482.24it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 484.70it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 512.21it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 506.37it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 514.12it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 491.73it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 495.82it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 473.69it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 460.37it/s]\n",
      " 35%|███▍      | 52/150 [00:00<00:00, 515.36it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 4e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 492.66it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 506.72it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 504.98it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 510.79it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 492.45it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 460.78it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 477.88it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 517.49it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 444.23it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 474.77it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 477.02it/s]\n",
      " 32%|███▏      | 48/150 [00:00<00:00, 476.39it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 5e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 462.39it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 460.65it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 298.15it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 387.70it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 430.58it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 477.23it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 480.20it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 475.34it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 485.69it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 472.97it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 478.08it/s]\n",
      " 33%|███▎      | 50/150 [00:00<00:00, 492.43it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 6.000000000000001e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 477.17it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 478.49it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 485.06it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 467.40it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 478.21it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 480.72it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 472.55it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 465.45it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 457.18it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 502.47it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 500.34it/s]\n",
      " 34%|███▍      | 51/150 [00:00<00:00, 502.78it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 7.000000000000001e-05 Accuracy on training set: 60.666668\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 493.66it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 454.05it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 470.78it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 481.09it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 499.85it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 480.83it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 479.97it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 427.20it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 476.83it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 490.59it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 510.86it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning Rate: 8e-05 Accuracy on training set: 60.666668\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# print(f\"Accuracy on training set: {check_accuracy(train_iterator, model)*100:2f}\")\n",
    "# print(f\"Accuracy on test set: {check_accuracy(test_iterator, model)*100:.2f}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}